# Terraform â€” AWS Glue ETL (Customers + Orders) â†’ Parquet (Silver/Gold) com Job Bookmarks e agendamento

Este repositÃ³rio provisiona (via **Terraform**) um pipeline simples de **ETL com AWS Glue (Spark)** usando **AWS Glue Studio (Visual Job)**.

A ideia Ã© ter dados em **CSV no S3 (bronze)**, transformar/enriquecer no Glue e gravar o resultado em **Parquet no S3** (camadas *silver/gold*), com **agendamento** e **Job Bookmarks** para evitar reprocessamento.

---

## âœ… O que este projeto cria

### AWS (via Terraform)
- **1 bucket S3** (com versioning e criptografia) para:
  - **dados bronze** (CSV): `s3://<bucket>/<bronze_prefix>/...`
  - **scripts do Glue**: `s3://<bucket>/<scripts_prefix>/glue_job.py`
  - **tmp**: `s3://<bucket>/tmp/`
- Upload de **arquivos de exemplo**:
  - `customers_*.csv` em `.../<bronze_prefix>/customers/ano=YYYY/mes=MM/dia=DD/`
  - `orders_*.csv` em `.../<bronze_prefix>/orders/ano=YYYY/mes=MM/dia=DD/`
- **IAM Role** para o Glue Job, incluindo:
  - leitura/escrita no bucket do projeto
  - logs no CloudWatch
  - **leitura do bucket de transforms do Glue Studio** (`aws-glue-studio-transforms-510798373988-prod-us-east-1`) â€” necessÃ¡rio para mÃ³dulos `gs_*` usados pelo job
- **AWS Glue Job** (Spark, `glueetl`) apontando para o script enviado ao S3
- **AWS Glue Trigger** agendado para rodar o job a cada **15 minutos** (cron)

> ObservaÃ§Ã£o: o arquivo `MeuETL_ImportIU-ConsoleAWS.json` estÃ¡ no repositÃ³rio como referÃªncia do Visual Job (DAG) exportado do Glue Studio.

---

## ðŸ§± Fluxo do pipeline

1) **Extract (S3 / CSV)**
- LÃª `customers` e `orders` (CSV) do S3

2) **Transform (Glue / Spark)**
- Enriquecimento e limpeza (join, filtros, agregaÃ§Ãµes etc.)

3) **Load (S3 / Parquet)**
- Grava resultado em Parquet (compressÃ£o) e atualiza/usa Glue Data Catalog (quando habilitado no script)

---

## ðŸ”„ TransformaÃ§Ãµes usadas (Glue Studio / Script)

Abaixo, um resumo das transformaÃ§Ãµes que este job aplica (no Visual Job e no script em `scripts/glue_job.py`):

1. **Filter (clientes ACTIVE)**  
   Filtra apenas clientes com `status == "ACTIVE"` (ex.: `Filter.apply(...)`).

2. **ApplyMapping (Rename keys for Join)**  
   Renomeia/ajusta schema das colunas de customers para evitar conflito no join (ex.: prefixo `cli_`).

3. **Join (orders + customers)**  
   Faz o join por `customer_id` para enriquecer orders com dados do cliente.

4. **DropFields (remover PII)**  
   Remove colunas sensÃ­veis como telefone/endereÃ§o (PII).

5. **Custom Transform (PySpark)**  
   TransformaÃ§Ã£o personalizada para:
   - `trim()` em campos (`cli_email`, `cli_name`)
   - filtro de qualidade: manter apenas registros com email **nÃ£o nulo** e **nÃ£o vazio**

6. **ConversÃ£o/FormataÃ§Ã£o de timestamp (`order_ts`)**
   - `gs_to_timestamp` (autodetect / conversÃ£o para timestamp)
   - `gs_format_timestamp` (ex.: gerar `order_ts_formatado` com `yyyy-MM-dd`)

   > Esses mÃ³dulos (`gs_to_timestamp.py`, `gs_format_timestamp.py`, `gs_common.py`) sÃ£o bibliotecas do Glue Studio e precisam estar configuradas como **Python library path** (em Terraform isso vira `--extra-py-files`).

7. **Filter (orders PAID)**  
   MantÃ©m apenas pedidos com status `PAID`.

8. **Aggregate (RelatÃ³rio de Vendas / Gold)**
   AgregaÃ§Ã£o (group by) para gerar mÃ©tricas de vendas (ex.: por estado/canal/data), com **sum**/**count**.

9. **Evaluate Data Quality**
   Executa regras simples de qualidade (ex.: `ColumnCount > 0`) e publica resultados.

---

## ðŸ§© PrÃ©-requisitos

- **Terraform** >= 1.6
- Credenciais AWS configuradas (ex.: `aws configure`, perfil/role)
- PermissÃ£o para criar recursos: S3, IAM, Glue (e Billing nÃ£o Ã© necessÃ¡rio)

---

## ðŸš€ Como executar

### 1) Clone e ajuste variÃ¡veis
Edite `variables.tf` ou crie `terraform.tfvars`:

```hcl
aws_region     = "us-east-1"
bucket_name    = "meu-bucket-unico-aqui"
bronze_prefix  = "bronze"
scripts_prefix = "scripts"
glue_job_name  = "MeuETL1"
```

### 2) Provisionar
```bash
terraform init
terraform apply
```

### 3) Colocar novos CSVs (bronze)
FaÃ§a upload de novos arquivos seguindo o padrÃ£o de prefixo. Exemplo:

```bash
aws s3 cp customers_novo.csv s3://<bucket>/<bronze_prefix>/customers/ano=2026/mes=01/dia=02/customers_20260102T120000Z_abcd.csv
aws s3 cp orders_novo.csv    s3://<bucket>/<bronze_prefix>/orders/ano=2026/mes=01/dia=02/orders_20260102T120000Z_abcd.csv
```

> **Boas prÃ¡ticas para Job Bookmarks**: evite sobrescrever o mesmo objeto no S3; prefira arquivos com nomes Ãºnicos (timestamp/uuid).

### 4) Rodar o job
- Manual: Glue Console â†’ Job â†’ **Run**
- AutomÃ¡tico: o trigger roda a cada **15 minutos** (UTC)

---

## ðŸ§ª Como validar o resultado

1) S3: verifique o prefixo de saÃ­da configurado no script do Glue (sink)  
2) Glue Console: acompanhe em **Runs** (DPU, tempo, logs)
3) CloudWatch Logs: ver logs do job

---

## ðŸ§· Sobre o erro `ModuleNotFoundError: gs_format_timestamp`

Se vocÃª criar o job via Terraform e receber:
`ModuleNotFoundError: No module named 'gs_format_timestamp'`

isso indica que o job nÃ£o recebeu as libs do Glue Studio. SoluÃ§Ã£o:
- garantir que o job tenha `--extra-py-files` apontando para:
  - `s3://aws-glue-studio-transforms-510798373988-prod-us-east-1/gs_common.py`
  - `s3://aws-glue-studio-transforms-510798373988-prod-us-east-1/gs_to_timestamp.py`
  - `s3://aws-glue-studio-transforms-510798373988-prod-us-east-1/gs_format_timestamp.py`
- e que a role do Glue tenha `s3:GetObject` nesse bucket (este projeto jÃ¡ inclui a policy)


## Glue Job (Visual)

Abaixo estÃ¡ a representaÃ§Ã£o **visual** do Glue Job (Glue Studio), que pode ser recriada/importada a partir do arquivo:

- [`MeuETL_ImportIU-ConsoleAWS.json`](MeuETL_ImportIU-ConsoleAWS.json)

### ObservaÃ§Ãµes

- O **Terraform** cria e mantÃ©m o Glue Job principalmente via **script Python + parÃ¢metros** (infra como cÃ³digo).
- A visualizaÃ§Ã£o em **IU (Glue Studio Visual)** **nÃ£o Ã© gerenciada diretamente pelo Terraform**.
- Se vocÃª editar o job **diretamente como script Python** (fora do modo visual), o Glue Studio pode **nÃ£o conseguir manter/reconpor o DAG visual** automaticamente.



# Consulta Athena

![relatorio_vendas_athena.png](relatorio_vendas_athena.png)

## ðŸ§¹ Cleanup

```bash
terraform destroy
```

> Dica: este projeto usa `force_destroy = true` no bucket (se habilitado), entÃ£o os objetos podem ser apagados ao destruir.

---

## ðŸ“š ReferÃªncias (AWS)

- AWS Glue â€” Job Bookmarks: https://docs.aws.amazon.com/glue/latest/dg/programming-etl-connect-bookmarks.html
- AWS Glue Studio â€” Visual jobs e transforms: https://docs.aws.amazon.com/glue/latest/ug/edit-job-in-glue-studio.html
- AWS Glue â€” Job parameters (`--extra-py-files` / `--job-bookmark-option`): https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html
- AWS Glue - PermissÃµes Glue Python modules - https://docs.aws.amazon.com/pt_br/glue/latest/dg/getting-started-min-privs-job.html
