# Terraform ‚Äî AWS Glue ETL (CSV ‚Üí Parquet ‚ÄúSilver‚Äù) com agendamento, Bookmarks e Partition Projection (Athena)

Este reposit√≥rio provisiona (via **Terraform**) um pipeline simples de **ETL em AWS Glue**:

- **Bronze (S3 / CSV)**: voc√™ coloca arquivos CSV em um prefixo de entrada no S3.
- **Glue Job (Spark)**: l√™ os CSVs, remove colunas, adiciona uma coluna de parti√ß√£o `dt` (data de ingest√£o) e grava em Parquet.
- **Silver (S3 / Parquet particionado)**: sa√≠da em `.../dt=YYYY-MM-DD/` com compress√£o.
- **Agendamento**: o job roda a cada **15 minutos** via **AWS Glue Trigger**.
- **Job Bookmarks**: evita reprocessar arquivos antigos quando o job roda novamente.
- **Partition Projection (Athena)**: a tabela no Athena/Glue Catalog pode ser configurada para **n√£o precisar registrar parti√ß√µes** (sem crawler, sem MSCK REPAIR).

---

## üß≠ Arquitetura (alto n√≠vel)

1. Upload de CSV no S3 (prefixo **bronze**)
2. Glue Job (Spark) processa somente ‚Äúnovos‚Äù dados (Bookmarked)
3. Escrita em Parquet no S3 (prefixo **silver**) particionado por `dt`
4. Athena consulta a Silver via **Partition Projection**

---

## ‚úÖ O que o Terraform cria

- 1 bucket S3 (ou reutiliza o nome definido) + ‚Äúpastas‚Äù/prefixos (`bronze`, `silver`, `tmp`, `scripts`)
- Upload do script do Glue (`scripts/glue_job.py`) para o S3
- IAM Role/Policy para o Glue Job (acesso ao S3 + logs)
- AWS Glue Catalog **Database** (para padroniza√ß√£o do projeto)
- AWS Glue **Job** (Spark) com par√¢metros + Job Bookmarks habilitado
- AWS Glue **Trigger** agendado a cada 15 minutos (cron em UTC)

> Observa√ß√£o: **no modo Partition Projection**, este projeto N√ÉO cria/atualiza a tabela/parti√ß√µes no Glue Catalog durante a execu√ß√£o do Job.
> A tabela deve ser criada no Athena (ou via Terraform com `aws_glue_catalog_table`) com as propriedades de projection.

---

## üì¶ Estrutura do reposit√≥rio

- `main.tf` ‚Äî Glue Job + Trigger
- `s3.tf` ‚Äî bucket + upload do script
- `iam.tf` ‚Äî role/policies do Glue
- `variables.tf` ‚Äî vari√°veis do projeto
- `scripts/glue_job.py` ‚Äî script ETL (CSV ‚Üí Parquet particionado)

---

## üîß Pr√©-requisitos

- Terraform instalado
- Credenciais AWS configuradas (perfil ou env vars)
- Permiss√£o para criar recursos: S3, IAM, Glue, CloudWatch Logs

---

## ‚ñ∂Ô∏è Como executar

```bash
terraform init
terraform apply
```

Ap√≥s o `apply`, envie um CSV para o prefixo **bronze** configurado (ex.: `bronzer/customers/`):

```bash
aws s3 cp ./customers.csv s3://<SEU_BUCKET>/<BRONZE_PREFIX>
```

O job roda automaticamente a cada 15 min. Para rodar na hora:

```bash
aws glue start-job-run --job-name <NOME_DO_JOB>
```

---

## üßæ Par√¢metros do Glue Job (Job Arguments)

O job recebe estes par√¢metros (definidos em `default_arguments` no Terraform):

- `--BUCKET` ‚Äî nome do bucket S3
- `--INPUT_PREFIX` ‚Äî prefixo bronze (onde chegam CSVs)
- `--OUTPUT_PREFIX` ‚Äî prefixo silver (onde ser√£o gravados os Parquets)
- `--DB_NAME` e `--TABLE_NAME` ‚Äî mantidos para padroniza√ß√£o/log, mas **n√£o atualizam o Catalog** no modo Projection

---

## üß† Sobre Job Bookmarks (somente ‚Äúarquivos novos‚Äù)

O Job Bookmark √© habilitado no job via:

- `--job-bookmark-option=job-bookmark-enable`

E, no script, a leitura do S3 usa `transformation_ctx`, requisito para bookmarks funcionarem corretamente.

Boas pr√°ticas para evitar reprocessamento inesperado:
- **n√£o sobrescreva** a mesma key no S3 (use nomes √∫nicos para cada arquivo)
- mantenha `max_concurrent_runs = 1`

---

## ü•à Sa√≠da Silver (Parquet particionado)

O job escreve em:

- `s3://<bucket>/<silver_prefix>/dt=YYYY-MM-DD/`

A parti√ß√£o `dt` √© a **data de ingest√£o (data da execu√ß√£o)**. Se voc√™ preferir particionar por data do arquivo (ex.: no nome), adapte o script.

---

## üîé Criando a tabela no Athena com Partition Projection

A Partition Projection evita ‚Äúregistrar parti√ß√µes‚Äù no Catalog. Voc√™ define as regras de parti√ß√£o na **propriedade da tabela**.

Abaixo um exemplo (ajuste **colunas** conforme seu schema final). Execute no Athena:

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS tutorial_glue.cliente_campos_dropados (
  -- TODO: ajuste as colunas reais do seu parquet
  customer_id string,
  name        string,
  email       string
)
PARTITIONED BY (dt string)
STORED AS PARQUET
LOCATION 's3://<SEU_BUCKET>/<SILVER_PREFIX>/'
TBLPROPERTIES (
  'projection.enabled'='true',

  'projection.dt.type'='date',
  'projection.dt.format'='yyyy-MM-dd',
  'projection.dt.range'='2025-01-01,NOW',
  'projection.dt.interval'='1',
  'projection.dt.interval.unit'='DAYS',

  'storage.location.template'='s3://<SEU_BUCKET>/<SILVER_PREFIX>/dt=${dt}/'
);
```

Exemplo de query:

```sql
SELECT *
FROM tutorial_glue.cliente_campos_dropados
WHERE dt = date_format(current_date, '%Y-%m-%d')
LIMIT 10;
```

---

## üß™ Troubleshooting r√°pido

- **Nada foi processado**: verifique se existe arquivo em `INPUT_PREFIX` e se o job est√° executando (CloudWatch Logs).
- **Reprocessou arquivo antigo**: voc√™ sobrescreveu a mesma key no S3 ou resetou o bookmark.
- **Athena n√£o encontra dados**: confira `LOCATION` e `storage.location.template` (precisam bater com o prefixo real).

---

## üßπ Cleanup

```bash
terraform destroy
```

---

## üìö Refer√™ncias (docs oficiais)

- AWS Glue ‚Äî Job Bookmarks: https://docs.aws.amazon.com/glue/latest/dg/programming-etl-connect-bookmarks.html
- AWS Glue ‚Äî Tracking processed data (bookmarks): https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html
- AWS Glue ‚Äî Job parameters (`--job-bookmark-option`): https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html
- Amazon Athena ‚Äî Partition Projection (vis√£o geral): https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html
- Amazon Athena ‚Äî Como configurar (pt-BR): https://docs.aws.amazon.com/pt_br/athena/latest/ug/partition-projection-setting-up.html
